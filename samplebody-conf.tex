\section{Introduction}
Mencoba tambahan
Semantic Role Labeling (SRL) is a task in Natural Language Processing (NLP) which aims to automatically assign semantic roles to each argument for each predicate in a given input sentence. As for a brief definition, given an input sentence, SRL system will give an output of “Who did what to whom” with what as the predicate and who and whom being the argument of the predicate. To illustrate, an example is given as follows.

Input: "I just ate fried chicken!"
Output:
-Predicate: eat
-Arguments:
a. Who: I
b. Whom: fried chicken

SRL is an integral part of understanding natural language as it helps machine to retrieve semantic information from the input. In the chat bot industry, the bots need to understand semantic information of user’s text in order to generate more personalized response. From the previous example, the bot could respond with “That’s great! how was the chicken?” since it knows that the user just ate a fried chicken. Furthermore, SRL has been widely used as one of the intermediate steps for many NLP tasks, some of which are information extraction [Bastianelli et. al, 2013], machine translation [Liu and Gildea 2010; Lo et al. 2013], question-answering [Dan and Lapata, 2007; Surdeanu et al., 2003, Moschitti et al., 2003].

There has been a lot of research on SRL for the past few years, especially for English formal language. The earlier research focuses on how to extract meaningful features out of syntactic or dependency parsers (Gildea et al. 2000, [Gildea et al. 2002]), [Pradhan et al. 2005].  This syntactic information plays a pivotal role in solving SRL problem for traditional systems as it addresses SLR’s long distance dependency. Unfortunately, this approach hardly depends on the linguistic experts experience assigning the correct syntactic information to the training data, which is costly. In order to address such problem, Jie Zhou et al. proposed an end-to-end learning of SRL using Recurrent Neural Networks (RNN) [x]. In their research, they utilized word embedding as the main feature and Deep Bi-Directional Long Short-Term Memory (DB-LSTM) for the RNN model. Their SRL system does not need any parsers since LSTM implicitly extracts the syntactic information over the sentences. The research result outperforms the previous state-of-the-art traditional SLR systems as it achieved F1 score of 81,07\%.

While formal language has been extensively studied, SRL on informal conversational language is yet to explore. The conversational language is heavily used on chatting platform. Since it is informal, the language has some characteristics including a wide variety of slangs and abbreviations, short sentences, and disorganized grammars. Since the grammars are often unstructured, one could not rely on syntactic parsers to build SRL system for conversational language. Moreover, daily conversations include non-sentential utterances, which could be tricky for the SRL task. These characteristics need deep understanding on how the SRL works towards informal conversational short text.

This work explores the SRL for informal conversational language, including creating a new set of semantic roles and proposing a new architecture called Context-Aware Bi-Directional Long Short-Term Memory Networks. We will deep dive into the semantic role characteristics found in informal Indonesian short text. We utilized word embedding and linguistic components as our main features. The SRL task was mainly evaluated on Indonesian informal conversational language used on chatting platform. Although this is a pilot task, we obtained a really promising result with F1 score of 74.48\%. In the end, we introduce a new task of understanding sequential conversation texts.

This paper is organized as follows. We first explain the previous works on SRL systems in chapter 2. In chapter 3, the methodology of the research is described, including the features and the model architectures being used. The results and analysis are then explained in chapter 4. Finally, we report our conclusion and potential future works in the last chapter.


\section{Previous Works}
SRL could be seen as either a classification or sequence labeling problem. The earlier research on SRL was conducted with the classification approach, meaning each argument is being predicted independently from the others. Most of the research used syntactic or dependency parsers from which many main features are extracted, such as path to predicate and constituent type. Gildea et al. proposed the first automatic semantic role labeling based on FrameNet [Gildea et al. 2000]. Then, Gildea et al. proposed the SRL based on PropBank data [Gildea et al. 2002]. There are also research using Support Vector Machine to implement SRL [Pradhan et al. 2005].

Such SRL system is heavily dependent on the quality of the parsers. The analysis done by Pradhan et al. (20XX) shows that most errors of the SRL system were caused by the parser errors. Furthermore, those parsers are costly to build, since it needs linguistic experts to annotate the data. If we want to use our model for another language, we should create the parsers specifically for it, which is another additional work.

To address this issue, Collobert et al. (2014) utilized deep learning for solving NLP tasks including Part-of-Speech Tagging, Chunking, and SRL with classification approach. The research aims to prove that one does not need any task-specific, hand-crafted features in order to achieve state-of-the-art performances. The word embedding thus is used as the main features across tasks, combined with Convolutional Neural Networks (CNN) architecture to train the models. They achieved promising results for the POS Tagging and Chunking, while for SRL they still need to use features from parsers to achieve robust performance.

Zhou et al. (2016) see SRL as a sequence labeling problem in which the arguments are labeled sequentially instead of independently. They see this as the nature of SRL that one argument’s role may affect the role of others. They used deep bi-directional long short-term memories (DB-LSTM) as the model architecture with the word embedding of the argument and respective predicate as the main features. The research shows that the performance of the sequence labeling approach using DB-LSTM is better than the classification approach using CNN. The research also shows that adding more stack of the DB-LSTM layer will make the model achieve better results.

In this work, we aim to use the context-aware Bi-LSTM designed with attention mechanism in order to capture context information of the sentence at higher, abstract level. While many of the previous research are using formal language, we see opportunity to explore the SRL on informal conversational language mainly used on chatting platform.

\section{Methodology}
In this chapter, we explain our research methodology including the data annotation, feature extraction, model architecture, experiments, and evaluation.

\subsection{Data Annotation}
Since there seems to be no resource available for labeled conversational language data, we annotated our own dataset of Indonesian text chats from Kata.ai’s bots. The data was annotated by three linguists. In this work, we create a new set of semantic roles mainly crafted for informal conversational language. The semantic roles proposed are explained in Table~\ref{tab:semantic_roles}.

\begin{table}
	\caption{Set of Semantic Roles for Conversational Language}
	\label{tab:semantic_roles}
	\begin{tabular}{ll}
		\toprule
		Semantic Roles		&Example\\
		\midrule
		AGENT				& \emph{I} brought you a present\\
		PATIENT				& I brought you \emph{a present}\\
		BENEFICIARY			& I brought \emph{you} a present\\
		GREET 				& Hi \emph{Andy}! I brought you a present\\
		MODAL 				& I \emph{can} eat at home today \\
		LOCATION 			& I can eat at \emph{home} today \\
		TIME 				& I can eat at home \emph{today} \\
		\bottomrule
	\end{tabular}
\end{table}

The main difference of this set of semantic roles compared to the previous ones is “GREET”. In conversational language, we often call the name of person we are talking to. This information is useful, for instance, we could derive that “you” refers to “Andy” in “Hi Andy! I brought you a present”.

It is worth to note that conversational language has unique characteristics. First, they use slangs and abbreviations. For example, one might use “u” instead of “you” in “I brought u a present”. Furthermore, the grammars are unstructured, for instance, “lagi makan aku” (“am eating I”). The sentences are also filled with interjections such as laugh “hahaha” and “lol”. Lastly, informal conversational sentences are really short, averaging around 4-6 words per sentence, hence it contains low information. These are the interesting challenges the SRL system should learn and tackle.

\subsection{Task Definition}
We use sequence labeling dan definisi umum di mana bagian elemen dari (Agen, Pasien. d;;)

\subsection{Features}
We utilized word embedding and POS tag as our main features with neighbouring words as the secondary feature. While most of the SRL research used predicate information as one of the main features, we omit to use it since we seek for an end-to-end SRL system which finds the predicate from scratch alongside with other semantic roles.

We train the word embedding model with our chat corpus using Word2Vec’s Skipgram architecture (Mikolov et al., 2013). The corpus contains X tokens. All the words were lowercased before being fed into the model. The trained word embedding model contains vocabulary size of X. The context window and word dimension parameters used are 5 and 32, respectively.

We use the gold-standard POS tag on our data as the second feature. We argue that POS tag is still important for building a robust model. Nonetheless, building POS tags model is way easier than building syntactic or dependency parsers.

As an additional feature, we also experiment with neighbouring words features with context window of 3. Suppose that we are in t timestep, the neighbouring words are the word embeddings of timestep t-1 and t+1. This feature could be useful for capturing the context of the word by looking at the words around it.

\subsection{Context-Aware BI-LSTM}
Recurrent Neural Networks (RNN) has a nature advantage for solving sequence labeling problem (Zhou et al., 2015), which is suitable for the SRL task. Zhou et al. used the so-called deep bi-directional long short-term memories, as part of RNN family, for their SRL system. However, the semantic role of an argument is highly dependent on the context of the sentence. Zhou et al. used predicate as the context information to help determine the semantic role of each argument. In our work, we did not use the predicate as our feature since we also want the machine to predict it.

We thus propose a new RNN architecture named Context-Aware Bi-Directional Long Short-Term Memories Networks (CABI-LSTM). The rationale is to add a dense yet useful information containing a sentence context to every time step in order to help the machine decide the semantic roles better. With this in mind, we build attention mechanism on top of the recurrent networks layers, as illustrated in Figure X. The attention mechanism firstly collects the context information by multiplying trainable weights with all the vectors from every time step of the last LSTM output. We sum all the weighted vectors and feed it into tanh layer. The output of the tanh layer is then transformed into distributed weights with a sum of 1 by a softmax layer. The original output vectors of the last LSTM output are multiplied by these distributed weights respectively. We then sum all the multiplication results as the final context information. All LSTM outputs are concatenated with this context information before going to the last softmax layer to predict the semantic roles.

The formula of this attention mechanism is described as follows.
(W21, W22, …, W2t) = softmax(tanh(W1. V1 + W2. V2 + … + Wt Vt))
Z = W21. V1 + W22. V2 + … + W2t. Vt

With V1, V2, …, Vt as the last LSTM output vectors, W1, W2, Wt as the trainable weights, W21, W22, W2t as the final weight of every time step. The context information is denoted as Z. Z is concatenated with all the vectors V1, V2, .., Vt as the additional information to predict the semantic roles in the last softmax output layer.

\section{Experiments}
In this chapter, we focus on presenting our experiment results and the analysis accordingly. There are two set of scenarios. The first scenario set aims to find the best combination of features. This scenario consists of four combinations as follows:

\begin{enumerate}
	\item Word Embedding (WE)
	\item Word Embedding + POS Tag (WE + POS)
	\item Word Embedding + Word Embedding of Neighbors (WE + WE-N)
	\item Word Embedding + POS Tag + Word Embedding of Neighbors (WE + POS + WE-N)
\end{enumerate}

The second scenario set evaluates two different architectures, which are the original BI-LSTM and CABI-LSTM. Underlying both architectures is a Convolutional Neural Network (CNN) layer, in order to catch more information surrounding each time step. This second scenario also aims to see the effect of hyper-parameter tuning for the CABI-LSTM.

\subsection{Evaluation Metrics}
As for evaluation, we use precision, recall and F1 metrics for all scenarios. The results are evaluated with partial match approach (XX et al., 20XX).

\subsection{Feature Combination Scenario}
Table~\ref{tab:feature_scenario} shows the scenario results of four feature combinations. The highest result is achieved with combination of WE + POS, followed by WE + WE-N + POS, WE, and WE + WE-N, with F1 scores of 72.29\%, 72.22\%, 62.00\%, and 61.92\% respectively. From these results, we can see the big impact POS Tag contributes for the performance. Using POS Tag enhances the result up to 10.29\%, when we compare WE + POS and WE combinations. The explanation would be the fact that POS Tag contains meaningful information such as Noun and Adjective which describes the word, it thus supports the word embedding feature.

Surprisingly, when we combine the neighboring words of the argument as in WE + WE-N, the result slightly decreases by 0.08\%, compared to only using WE feature. This is also the case when we compare WE + WE-N with WE + WE-N + POS scenarios, which decreases by 0.07\% when we used neighboring words. It tells us that neighbouring words do not improve the performance at all. We suggest that this is because the CNN layer already extracts these information implicitly, by capturing surrounding information and compressed it into one vector. Hence, we do not need an explicit neighboring words as part of our features. 

\begin{table}
	\caption{Results of Feature Combination Scenario}
	\label{tab:feature_scenario}
	\begin{tabular}{llll}
		\toprule
		Features		&Precision	&Recall		&F1			\\
		\midrule
		WE				&	64.68\%				&	60.25\%				&	62.00\%	\\
		WE + POS		&	74.24\%				&	\textbf{71.26\%}	&	\textbf{72.29\%}	\\
		WE + WE-N		&	64.17\%				&	60.29\%				&	61.92\%	\\
		WE + POS + WE-N	&	\textbf{74.51\%}	&	70.69\%				&	72.23\%	\\
		\bottomrule
	\end{tabular}
\end{table}

Since WE + POS outputs the best result in terms of F1 score in this scenario set, we will use it for the next set of scenarios.

\subsection{Model Architecture Scenario}
The experiment results of the second scenario set are presented in Table~\ref{tab:architecture_scenario}. The results show that the CABI-LSTM architecture outperforms the original BI-LSTM architecture. The highest F1 is achieved by CABI-LSTM 256. The most drastic improvement here is the precision. The precision started to increase with dimension 128 by 2.00\%, compared to the original BI-LSTM (76.25\% vs 74.24\%). When we added more dimension to the weight, 256, the precision increases by 3.10\%, compared to the original one. When the system is more context-aware to predict every time step, it becomes more careful when predicting a label. Hence, the number of precision increases, since when it predicts, it predicts carefully. 

The best recall is on the weight 128with 73.52\%. It increases by 2.2\%.

\begin{table}
	\caption{Results of Model Architecture Scenario}
	\label{tab:architecure_scenario}
	\begin{tabular}{llll}
		\toprule
		Name			&Precision					&Recall		&F1			\\
		\midrule
		BI-LSTM				&	74.24\%				&	71.26\%				&	72.29\%	\\
		CABI-LSTM-64		&	73.37\%				&	73.25\%				&	73.05\%	\\
		CABI-LSTM-128		&	76.25\%				&	\textbf{73.52\%}	&	74.05\%	\\
		CABI-LSTM-256		&	\textbf{77.33\%}	&	73.10\%				&	\textbf{74.78\%}\\
		\bottomrule
	\end{tabular}
\end{table}


\subsection{Type Changes and {\itshape Special} Characters}

We have already seen several typeface changes in this sample.  You can
indicate italicized words or phrases in your text with the command
\texttt{{\char'134}textit}; emboldening with the command
\texttt{{\char'134}textbf} and typewriter-style (for instance, for
computer code) with \texttt{{\char'134}texttt}.  But remember, you do
not have to indicate typestyle changes when such changes are part of
the \textit{structural} elements of your article; for instance, the
heading of this subsection will be in a sans serif\footnote{Another
  footnote here.  Let's make this a rather long one to see how it
  looks.} typeface, but that is handled by the document class file.
Take care with the use of\footnote{Another footnote.}  the
curly braces in typeface changes; they mark the beginning and end of
the text that is to be in the different typeface.

You can use whatever symbols, accented characters, or non-English
characters you need anywhere in your document; you can find a complete
list of what is available in the \textit{\LaTeX\ User's Guide}
\cite{Lamport:LaTeX}.

\subsection{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of
the three are discussed in the next sections.

\subsubsection{Inline (In-text) Equations}
A formula that appears in the running text is called an
inline or in-text formula.  It is produced by the
\textbf{math} environment, which can be
invoked with the usual \texttt{{\char'134}begin\,\ldots{\char'134}end}
construction or with the short form \texttt{\$\,\ldots\$}. You
can use any of the symbols and structures,
from $\alpha$ to $\omega$, available in
\LaTeX~\cite{Lamport:LaTeX}; this section will simply show a
few examples of in-text equations in context. Notice how
this equation:
\begin{math}
  \lim_{n\rightarrow \infty}x=0
\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsubsection{Display Equations}
A numbered display equation---one set off by vertical space from the
text and centered horizontally---is produced by the \textbf{equation}
environment. An unnumbered display equation is produced by the
\textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols
and structures available in \LaTeX\@; this section will just
give a couple of examples of display equations in context.
First, consider the equation, shown as an inline equation above:
\begin{equation}
  \lim_{n\rightarrow \infty}x=0
\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}
  \sum_{i=0}^{\infty} x + 1
\end{displaymath}
and follow it with another numbered equation:
\begin{equation}
  \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\subsection{Citations}
Citations to articles~\cite{bowman:reasoning,
clark:pct, braams:babel, herlihy:methodology},
conference proceedings~\cite{clark:pct} or maybe
books \cite{Lamport:LaTeX, salas:calculus} listed
in the Bibliography section of your
article will occur throughout the text of your article.
You should use BibTeX to automatically produce this bibliography;
you simply need to insert one of several citation commands with
a key of the item cited in the proper location in
the \texttt{.tex} file~\cite{Lamport:LaTeX}.
The key is a short reference you invent to uniquely
identify each work; in this sample document, the key is
the first author's surname and a
word from the title.  This identifying key is included
with each item in the \texttt{.bib} file for your article.

The details of the construction of the \texttt{.bib} file
are beyond the scope of this sample document, but more
information can be found in the \textit{Author's Guide},
and exhaustive details in the \textit{\LaTeX\ User's
Guide} by Lamport~\shortcite{Lamport:LaTeX}.


This article shows only the plainest form
of the citation command, using \texttt{{\char'134}cite}.

\subsection{Tables}
ensure this proper ``floating'' placement of tables, use the


It is strongly recommended to use the package booktabs~\cite{Fear05}
and follow its main principles of typography with respect to tables:
\begin{enumerate}
\item Never, ever use vertical rules.
\item Never use double rules.
\end{enumerate}
It is also a good idea not to overuse horizontal rules.


\subsection{Figures}

Like tables, figures cannot be split across pages; the best placement
for them is typically the top or the bottom of the page nearest their
initial cite.  To ensure this proper ``floating'' placement of
figures, use the environment \textbf{figure} to enclose the figure and
its caption.

This sample document contains examples of \texttt{.eps} files to be
displayable with \LaTeX.  If you work with pdf\LaTeX, use files in the
\texttt{.pdf} format.  Note that most modern \TeX\ systems will convert
\texttt{.eps} to \texttt{.pdf} for you on the fly.  More details on
each of these are found in the \textit{Author's Guide}.

\begin{figure}
\includegraphics{fly}
\caption{A sample black and white graphic.}
\end{figure}

\begin{figure}
\includegraphics[height=1in, width=1in]{fly}
\caption{A sample black and white graphic
that has been resized with the \texttt{includegraphics} command.}
\end{figure}


As was the case with tables, you may want a figure that spans two
columns.  To do this, and still to ensure proper ``floating''
placement of tables, use the environment \textbf{figure*} to enclose
the figure and its caption.  And don't forget to end the environment
with \textbf{figure*}, not \textbf{figure}!

\begin{figure*}
\includegraphics{flies}
\caption{A sample black and white graphic
that needs to span two columns of text.}
\end{figure*}


\begin{figure}
\includegraphics[height=1in, width=1in]{rosette}
\caption{A sample black and white graphic that has
been resized with the \texttt{includegraphics} command.}
\end{figure}

\subsection{Theorem-like Constructs}

Other common constructs that may occur in your article are the forms
for logical constructs like theorems, axioms, corollaries and proofs.
ACM uses two types of these constructs:  theorem-like and
definition-like.

Here is a theorem:
\begin{theorem}
  Let $f$ be continuous on $[a,b]$.  If $G$ is
  an antiderivative for $f$ on $[a,b]$, then
  \begin{displaymath}
    \int^b_af(t)\,dt = G(b) - G(a).
  \end{displaymath}
\end{theorem}

Here is a definition:
\begin{definition}
  If $z$ is irrational, then by $e^z$ we mean the
  unique number that has
  logarithm $z$:
  \begin{displaymath}
    \log e^z = z.
  \end{displaymath}
\end{definition}

The pre-defined theorem-like constructs are \textbf{theorem},
\textbf{conjecture}, \textbf{proposition}, \textbf{lemma} and
\textbf{corollary}.  The pre-defined de\-fi\-ni\-ti\-on-like constructs are
\textbf{example} and \textbf{definition}.  You can add your own
constructs using the \textsl{amsthm} interface~\cite{Amsthm15}.  The
styles used in the \verb|\theoremstyle| command are \textbf{acmplain}
and \textbf{acmdefinition}.

Another construct is \textbf{proof}, for example,

\begin{proof}
  Suppose on the contrary there exists a real number $L$ such that
  \begin{displaymath}
    \lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
  \end{displaymath}
  Then
  \begin{displaymath}
    l=\lim_{x\rightarrow c} f(x)
    = \lim_{x\rightarrow c}
    \left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
    = \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
    \frac{f(x)}{g(x)} = 0\cdot L = 0,
  \end{displaymath}
  which contradicts our assumption that $l\neq 0$.
\end{proof}

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate


\paragraph{Inline (In-text) Equations}
\paragraph{Display Equations}


\begin{acks}
  The authors would like to thank Dr. Yuhua Li for providing the
  matlab code of  the \textit{BEPS} method. 

  The authors would also like to thank the anonymous referees for
  their valuable comments and helpful suggestions. The work is
  supported by the \grantsponsor{GS501100001809}{National Natural
    Science Foundation of
    China}{http://dx.doi.org/10.13039/501100001809} under Grant
  No.:~\grantnum{GS501100001809}{61273304}
  and~\grantnum[http://www.nnsf.cn/youngscientsts]{GS501100001809}{Young
    Scientsts' Support Program}.

\end{acks}
